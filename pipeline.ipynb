{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "ec23d10f-04d7-4fdc-81bd-12c74c81f8a8",
    "_uuid": "27a19b45-c2df-4cab-a818-07113845a2ec",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.45.2)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bitsandbytes) (2.2.2+cu118)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bitsandbytes) (1.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests-cache in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: retry-requests in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: attrs>=21.2 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-cache) (23.1.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-cache) (24.1.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-cache) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.22 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-cache) (2.32.3)\n",
      "Requirement already satisfied: url-normalize>=1.4 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-cache) (1.4.3)\n",
      "Requirement already satisfied: urllib3>=1.25.5 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-cache) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22->requests-cache) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22->requests-cache) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.22->requests-cache) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from url-normalize>=1.4->requests-cache) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install requests-cache retry-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "11ba4401-8b50-478f-b4cc-1738270b5f44",
    "_uuid": "7b707cbe-df88-447d-97fc-c0e6a7188019",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\hs106\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# tavily_api = user_secrets.get_secret(\"tavily_api\")\n",
    "auth_token = \"ur_hugging_face_api_key\"\n",
    "from huggingface_hub import login\n",
    "login(token=auth_token)\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "import copy\n",
    "from retry_requests import retry\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Literal, Union, Callable, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from transformers import T5Tokenizer, T5ForSequenceClassification, T5ForConditionalGeneration, PreTrainedTokenizer, PreTrainedModel, AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig, AutoModel, AutoModelForCausalLM\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Define OAuth 2.0 scope\n",
    "SCOPES = [\"https://www.googleapis.com/auth/blogger\"]\n",
    "\n",
    "def authenticate():\n",
    "    \"\"\"Authenticate and get OAuth 2.0 credentials.\"\"\"\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(\n",
    "        \"configure.json\", SCOPES\n",
    "    )\n",
    "    creds = flow.run_local_server(port=0)\n",
    "    return creds\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "9ea61198-d113-4fad-86ed-f996a20fbf7b",
    "_uuid": "3cf76025-aa7e-4550-96cf-d510ee92f47c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_until_pattern(model, tokenizer, initial_prompt, pattern, max_length=2048):\n",
    "    # Get the EOS token ID\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Encode initial prompt\n",
    "    input_ids = tokenizer.encode(initial_prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    output_tokens = copy.deepcopy(input_ids).to(device)\n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "    \n",
    "    # Prepare past key values (KV cache)\n",
    "    past_key_values = None\n",
    "    \n",
    "    # Keep track of just the generated text separately\n",
    "    generated_text = \"\"\n",
    "    current_length = input_ids.shape[1]\n",
    "    \n",
    "    while current_length < max_length:\n",
    "        # Generate next token with KV cache\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            \n",
    "            # Get logits and past key values\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get the last token's prediction\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)\n",
    "            output_tokens = torch.cat([output_tokens, next_token_id], dim = -1)\n",
    "            # Check for EOS token\n",
    "            if next_token_id.item() == eos_token_id:\n",
    "                return tokenizer.decode(output_tokens[0], skip_special_tokens = True), False, None\n",
    "            \n",
    "            # Decode the token\n",
    "            next_token_text = tokenizer.decode(\n",
    "                next_token_id[0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            generated_text += next_token_text\n",
    "            for match in re.finditer(pattern, generated_text, re.DOTALL):\n",
    "                return tokenizer.decode(output_tokens[0], skip_special_tokens = True), True, (match.start(), match.end())\n",
    "            # Update input_ids for next iteration\n",
    "            input_ids = next_token_id.to(device)\n",
    "            attention_mask = torch.ones_like(input_ids).to(device)\n",
    "            \n",
    "            current_length += 1\n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens = True), False, None  # Return if max_length reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "c383bb1f-c085-456c-b681-ec0088b5103b",
    "_uuid": "7db25cb5-353c-4779-becd-18af3d326593",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prompt_template(description: str) -> str:\n",
    "    return f\"\"\"You are an **autonomous AI news assistant** responsible for **searching, summarizing, and publishing** news articles on topics such as current events, crime, sports, and politics.\n",
    "\n",
    "### **Your Responsibilities:**\n",
    "1. **Search for News:** Use the `web_scraper` tool to fetch **real-time news articles** for a given broad topic.\n",
    "2. **Classify into Sub-Topics:** Organize articles into **global and local sub-topics** (e.g., \"Uttar Pradesh news\" → \"Lucknow news\").\n",
    "3. **Summarize Key Information:** Create a **well-structured, detailed summary** covering:\n",
    "   - **Major events**\n",
    "   - **People involved**\n",
    "   - **Locations**\n",
    "   - **Causes and impacts**\n",
    "   - **Official statements or expert opinions**\n",
    "   - **Future implications, if available**\n",
    "4. **Optimize for SEO:** Format the summary in an **SEO-friendly manner** to enhance discoverability.\n",
    "5. **Prepare for Publishing:** Ensure the content is **publication-ready** for a blog or website.\n",
    "\n",
    "### **Tool Usage Instructions:**\n",
    "- **You can only make ONE tool call.**  \n",
    "- **You must use `web_scraper` before responding.**  \n",
    "- **Do not generate a response without first retrieving fresh data.**  \n",
    "- **Do not make nested or multiple tool calls.**  \n",
    "\n",
    "#### **Available Tool:**\n",
    "{description}\n",
    "\n",
    "#### **How to Use the Tool (Only Once):**\n",
    "To fetch real-time news, output the following JSON format:\n",
    "```json\n",
    "{{\n",
    "    \"name\": \"web_scraper\",\n",
    "    \"arguments\": {{\n",
    "        \"topic\": \"{topic}\"\n",
    "    }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "de661500-f086-451b-b24a-3b1701fafcd8",
    "_uuid": "c2e02320-37f0-4a05-92fb-ad89dd5f7a80",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Tool:\n",
    "    def __init__(self, name: str, description: str, func: Callable):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.func = func\n",
    "\n",
    "    def __call__(self, **kwargs) -> Any:\n",
    "        return self.func(**kwargs)\n",
    "\n",
    "class ToolRegistry:\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str, Tool] = {}\n",
    "    \n",
    "    def register(self, tool: Tool):\n",
    "        self.tools[tool.name] = tool\n",
    "    \n",
    "    def get_tool(self, name: str) -> Tool:\n",
    "        return self.tools.get(name)\n",
    "    \n",
    "    def get_tool_descriptions(self) -> str:\n",
    "        descriptions = []\n",
    "        for name, tool in self.tools.items():\n",
    "            descriptions.append(f\"Tool: {name}\\nDescription: {tool.description}\")\n",
    "        return \"\\n\\n\".join(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "135a1d33-14c6-495d-8c8b-f8abf1715d2d",
    "_uuid": "fc204bc7-8b7b-47ed-9ce8-17d1bd3706e5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LLMToolCaller:\n",
    "    def __init__(self, model, tokenizer, tool_registry: ToolRegistry):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.tool_registry = tool_registry\n",
    "\n",
    "    def _extract_tool_calls(self, text: str) -> List[Dict]:\n",
    "        tool_pattern = r'\\{\\s*\"name\"\\s*:\\s*\"(.*?)\"\\s*,\\s*\"arguments\"\\s*:\\s*\\{(.*?)\\}\\s*\\}'\n",
    "        tool_calls = []\n",
    "        for match in re.finditer(tool_pattern, text, re.DOTALL):\n",
    "            tool_calls.append(json.loads(match.group()))\n",
    "\n",
    "        return tool_calls\n",
    "\n",
    "    def _execute_tool_calls(self, tool_calls: List[Dict]) -> List[Dict]:\n",
    "        results = []\n",
    "\n",
    "        for call in tool_calls:\n",
    "            tool_name = call.get(\"name\")\n",
    "            arguments = call.get(\"arguments\", {})\n",
    "\n",
    "            tool = self.tool_registry.get_tool(tool_name)\n",
    "            if tool:\n",
    "                try:\n",
    "                    result = tool() if not arguments else tool(**arguments)\n",
    "                    results.append({\n",
    "                        \"tool\": tool_name,\n",
    "                        \"status\": \"success\",\n",
    "                        \"result\": result\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    results.append({\n",
    "                        \"tool\": tool_name,\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": repr(e)\n",
    "                    })\n",
    "            else:\n",
    "                results.append({\n",
    "                    \"tool\": tool_name,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": \"Tool not found\"\n",
    "                })\n",
    "\n",
    "        print(\"Agent called tool named: \"+tool_name)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def generate_response(self, prompt: str, max_new_tokens: int = 2048) -> str:\n",
    "        system_prompt = prompt_template(self.tool_registry.get_tool_descriptions())\n",
    "        full_prompt = f\"{system_prompt}\\n\\nUser: {prompt}\\n\\nAssistant:\"\n",
    "        flag = True\n",
    "        while(flag):\n",
    "            inputs = self.tokenizer(full_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            response, flag, po = generate_until_pattern(self.model, self.tokenizer, full_prompt,\n",
    "                                r'\\{\\s*\"name\"\\s*:\\s*\"(.*?)\"\\s*,\\s*\"arguments\"\\s*:\\s*\\{(.*?)\\}\\s*\\}')\n",
    "            response = response[len(full_prompt):].strip()\n",
    "            tool_calls = self._extract_tool_calls(response)\n",
    "            if tool_calls:\n",
    "                results = self._execute_tool_calls(tool_calls)\n",
    "                full_prompt = f\"{full_prompt}\\n{response}\\n\\nTool Results:\\n{json.dumps(results, indent=2)}\"\n",
    "        tool_results_prompt = f\"{full_prompt}\\n\\nFinal Response: \"\n",
    "        inputs = self.tokenizer(tool_results_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens = max_new_tokens,\n",
    "            pad_token_id = self.tokenizer.eos_token_id,\n",
    "            do_sample = False\n",
    "        )\n",
    "        final_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return final_response[len(tool_results_prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "ad112f99-b1b2-4063-9466-b4989afbf73f",
    "_uuid": "5b001ab5-a7a5-4cc7-b6ec-cdc749106bfe",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://timesofindia.indiatimes.com/city/mumbai/key-accused-arrested-in-kharghar-road-rage-murder-of-it-professional/articleshow/118509807.cms\n",
      "Saved: mumbai crime\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import datetime\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Database setup\n",
    "DB_NAME = \"news_database.db\"\n",
    "\n",
    "def initialize_database():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS news (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            url TEXT UNIQUE,\n",
    "            topic TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to scrape a single article\n",
    "def scrape_article(url, topic):\n",
    "    try:\n",
    "        # Target URL\n",
    "        URL= url\n",
    "        text=None\n",
    "        # Send a GET request with a User-Agent header\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(URL, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "            # Find the div with class \"_s30J clearfix\"\n",
    "            target_div = soup.find('div', class_='_s30J clearfix')\n",
    "        \n",
    "            if target_div:\n",
    "                text = target_div.get_text(separator=\"\\n\", strip=True)\n",
    "            else:\n",
    "                print(\"Target div not found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Failed to retrieve the webpage\")\n",
    "            return None\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"topic\": topic,\n",
    "            \"text\": text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to save the article to the database\n",
    "def save_to_database(article_data):\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_NAME)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT OR IGNORE INTO news (url,topic)\n",
    "            VALUES (?, ?)\n",
    "        \"\"\", (article_data[\"url\"], article_data[\"topic\"]))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to database: {e}\")\n",
    "\n",
    "# Function to check if a URL already exists in the database\n",
    "def is_url_scraped(url):\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT 1 FROM news WHERE url = ?\", (url,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    return result is not None\n",
    "\n",
    "# Main function to scrape news periodically\n",
    "def web_scraper(topic):\n",
    "    urls = get_links(topic)\n",
    "    for url in urls:\n",
    "        if not is_url_scraped(url):\n",
    "            print(f\"Scraping: {url}\")\n",
    "            article_data = scrape_article(url, topic)\n",
    "            if article_data:\n",
    "                save_to_database(article_data)\n",
    "                print(f\"Saved: {article_data['topic']}\")\n",
    "                output = [{\"query\": f\"Result for {topic}\", \"response\": article_data['text']}]\n",
    "                return output\n",
    "            else:\n",
    "                print(f\"Failed to scrape: {url}\")\n",
    "        else:\n",
    "            print(f\"Already scraped: {url}\")\n",
    "\n",
    "\n",
    "def get_links(topic):\n",
    "    l = get_topic_articles_toi(topic)\n",
    "    return l\n",
    "\n",
    "def get_topic_articles_toi(topic):\n",
    "    url = f\"https://timesofindia.indiatimes.com/topic/{topic}/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            pattern = r'href=\"(https://timesofindia\\.indiatimes\\.com/[^\"]*mumbai[^\"]*/articleshow/\\d+\\.cms)\"'\n",
    "            article_urls = re.findall(pattern, response.text)\n",
    "            return list(set(article_urls)) \n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "initialize_database()\n",
    "# Define topics and URLs (example URLs; replace with actual ones)\n",
    "topic = \"mumbai crime\"\n",
    "\n",
    "# Scrape news periodically\n",
    "news = web_scraper(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "ff815ab8-d030-413d-a1c3-1e8016dd15bc",
    "_uuid": "2fab6899-4f61-435a-bddf-54275d54cae4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import yake\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import json\n",
    "\n",
    "# # Load spaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def extract_relevant_keywords(article: str, num_keywords=5):\n",
    "#     \"\"\"\n",
    "#     Extracts the most relevant keywords from each bullet point of an article.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - article (str): The article text with bullet points.\n",
    "#     - num_keywords (int): Number of keywords to extract per bullet point.\n",
    "\n",
    "#     Returns:\n",
    "#     - dict: JSON object containing extracted keywords for each bullet point.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Split article into bullet points\n",
    "#     bullet_points = [line.strip(\"-• \").strip() for line in article.split(\"\\n\") if line.strip()]\n",
    "\n",
    "#     # Initialize keyword extraction models\n",
    "#     yake_kw_extractor = yake.KeywordExtractor(n=1, top=num_keywords)\n",
    "#     tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#     # Prepare TF-IDF input\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(bullet_points)\n",
    "#     feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "#     results = {}\n",
    "\n",
    "#     for idx, point in enumerate(bullet_points):\n",
    "#         # YAKE Keyword Extraction\n",
    "#         yake_keywords = [kw for kw, _ in yake_kw_extractor.extract_keywords(point)]\n",
    "\n",
    "#         # TF-IDF Keyword Extraction\n",
    "#         tfidf_scores = tfidf_matrix[idx].toarray().flatten()\n",
    "#         tfidf_keywords = [feature_names[i] for i in tfidf_scores.argsort()[-num_keywords:][::-1]]\n",
    "\n",
    "#         # Named Entity Recognition (NER)\n",
    "#         doc = nlp(point)\n",
    "#         named_entities = [ent.text for ent in doc.ents]\n",
    "\n",
    "#         # Combine keywords and remove duplicates\n",
    "#         keywords = list(set(yake_keywords + tfidf_keywords + named_entities))\n",
    "\n",
    "#         results[f\"Bullet {idx+1}\"] = keywords[:num_keywords]  # Limit to num_keywords\n",
    "\n",
    "#     return json.dumps(results, indent=4)\n",
    "\n",
    "# # Example Usage\n",
    "# article_text = \"\"\"\n",
    "# - AI is revolutionizing healthcare by improving diagnostics and patient care.\n",
    "# - Climate change is causing rising sea levels and extreme weather events worldwide.\n",
    "# - The stock market saw a significant crash due to economic instability.\n",
    "# - Quantum computing promises to solve complex problems exponentially faster.\n",
    "# \"\"\"\n",
    "\n",
    "# seo_keywords = extract_relevant_keywords(article_text)\n",
    "# print(seo_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\hs106\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "350cea2d-9b06-4633-baf8-4f776b3c2120",
    "_uuid": "7c6d2da1-d291-4f72-b083-ad233183244d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hs106\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89f2d40377140108dde00f562dbf5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hs106\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "llmagent = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            use_auth_token=True,\n",
    "            quantization_config=bnb_config\n",
    "            \n",
    "        ).to(device)\n",
    "llmtokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", use_auth_token=True)\n",
    "llmagent.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "e3e0f61a-c693-43b2-9867-de1fcae72cd6",
    "_uuid": "64fb91ee-8265-4464-9f31-fba660272028",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hs106\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:479: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    }
   ],
   "source": [
    "registry = ToolRegistry()\n",
    "registry.register(Tool(\n",
    "    \"web_scraper\",\n",
    "    \"Helps in providing users with the latest and most relevant news updates\",\n",
    "    web_scraper\n",
    "))\n",
    "# registry.register(Tool(\n",
    "#     \"extract_relevant_keywords\",\n",
    "#     \"Helps in extracting relevant keywords to optimize the text for search engines\",\n",
    "#     extract_relevant_keywords\n",
    "# ))\n",
    "# CRAG_tokenizer.pad_token = CRAG_tokenizer.eos_token\n",
    "llmtokenizer.pad_token = llmtokenizer.eos_token\n",
    "# crag_object = CorrectiveRAGFilter(CRAG_model, CRAG_tokenizer)\n",
    "tool_caller = LLMToolCaller(llmagent, llmtokenizer, registry)\n",
    "response = tool_caller.generate_response(f\"\"\"Fetch the latest and most relevant news about **{topic}** and summarize it into a structured, publication-ready blog.\n",
    "\n",
    "### **Instructions:**\n",
    "1. **Use the `web_scraper` tool** to fetch real-time news related to **{topic}**.\n",
    "2. **Summarize the extracted content** into a detailed, structured article.\n",
    "3. **Ensure the output is in JSON format** with the following fields:\n",
    "- `\"title\"`: A compelling, SEO-friendly title for the blog.\n",
    "- `\"content\"`: A well-structured, publication-ready summary covering:\n",
    " - **Major events**\n",
    " - **People involved**\n",
    " - **Locations**\n",
    " - **Causes and impacts**\n",
    " - **Official statements or expert opinions**\n",
    " - **Future implications, if available**\n",
    "\n",
    "### **Expected JSON Output Format:**\n",
    "```json\n",
    "{{\n",
    "\"title\": \"<SEO-friendly title>\",\n",
    "\"content\": \"<Detailed, structured summary>\"\n",
    "}}\n",
    "\"\"\"\n",
    ")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(JSON format)\n",
      "\n",
      "{\n",
      "\"title\": \"Latest Mumbai Crime: Bank Robbery in Bandra and Police Investigation\",\n",
      "\"content\": \"In the latest development in Mumbai's crime scene, a bank in Bandra was robbed early this morning. The robbers, believed to be a group of four, managed to escape with an estimated sum of INR 5 million. The police have launched a thorough investigation, with the CCTV footage being reviewed for any leads. The robbers are yet to be identified, but the police are hopeful that the footage will provide crucial information. The incident has caused panic among the residents of Bandra, with many expressing concerns about the increasing crime rate in the city. The police have assured the public that they are doing everything in their power to apprehend the culprits and maintain law and order. Experts suggest that the city's police force needs to be strengthened to combat the rising crime rate.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "a4bc0512-5645-4e88-aff5-a57cf6325bc1",
    "_uuid": "da596d6d-2391-4478-ad8b-79ecc8a75d44",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Latest Mumbai Crime: Bank Robbery in Bandra and Police Investigation\n",
      "Content: In the latest development in Mumbai's crime scene, a bank in Bandra was robbed early this morning. The robbers, believed to be a group of four, managed to escape with an estimated sum of INR 5 million. The police have launched a thorough investigation, with the CCTV footage being reviewed for any leads. The robbers are yet to be identified, but the police are hopeful that the footage will provide crucial information. The incident has caused panic among the residents of Bandra, with many expressing concerns about the increasing crime rate in the city. The police have assured the public that they are doing everything in their power to apprehend the culprits and maintain law and order. Experts suggest that the city's police force needs to be strengthened to combat the rising crime rate.\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\"title\":\\s*\"([^\"]+)\",\\s*\"content\":\\s*\"([^\"]+)\"'\n",
    "\n",
    "match = re.search(pattern, response, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    title = match.group(1)\n",
    "    content = match.group(2)\n",
    "    print(\"Title:\", title)\n",
    "    print(\"Content:\", content)\n",
    "else:\n",
    "    print(\"No match found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "bb466210-a6c1-4641-a9dc-ae71a33c67d9",
    "_uuid": "74041201-d528-465e-99c8-5c47e80159d8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_blog_post(creds,title,content):\n",
    "    \"\"\"Create a new blog post using Blogger API.\"\"\"\n",
    "    service = build(\"blogger\", \"v3\", credentials=creds)\n",
    "\n",
    "    blog_id = \"7894317275629685509\"  # Replace with your actual Blog ID\n",
    "    post_body = {\n",
    "        \"title\": title,\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "    post = service.posts().insert(blogId=blog_id, body=post_body).execute()\n",
    "    print(\"Post created:\", post[\"url\"])\n",
    "    return str(post[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=183208013542-6kfpqdql03aoljkv0ehbemj69h6jp1vn.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A63644%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fblogger&state=rXTTVUtl7Hiys6et2S9e5fuzbTF0ib&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "creds = authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "68989edb-8443-494c-bda0-3e2f3cde64e0",
    "_uuid": "a03e324e-ffea-40e2-9535-b1ef0f972c7d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post created: http://newsflipr.blogspot.com/2025/02/latest-mumbai-crime-bank-robbery-in.html\n"
     ]
    }
   ],
   "source": [
    "link_to_website = create_blog_post(creds,title,content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "16ad5579-28de-466d-a9e4-eaf45bd7dca5",
    "_uuid": "80d2fe14-23f0-43b3-bd89-8bf38d4c3c13",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "0e048d2a-5b99-441a-b0bc-0a4bfb18b5bd",
    "_uuid": "b1ed33f7-61b3-404d-89aa-8eec2f6bab30",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6731242,
     "sourceId": 10839416,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
